{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "LFW has limitations:\n",
    "    less woman\n",
    "    not much side pose and all! \n",
    "    trained on aligned faces! see.\n",
    "    not much of lighting variation\n",
    "    \n",
    "so will need to fine-tune on ours.\n",
    "    \n",
    "FaceNet: output hi embeddings instead of intermiedate layer! 128 dims. triplet loss\n",
    "    input changed to 96*96*3 \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "#code to flip upside down raw data\n",
    "\n",
    "import os\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "reverse_path = r\"C:\\Users\\Lenovo\\Desktop\\Project\\DataX-2\\Raw Data\\reverse\"\n",
    "\n",
    "write_folder = r\"C:\\Users\\Lenovo\\Desktop\\Project\\DataX-2\\Raw Data\\200108_00\" #path should exist!\n",
    "\n",
    "\n",
    "for img in os.listdir(reverse_path):\n",
    "    img_path = os.path.join(reverse_path,img)\n",
    "    image = cv2.imread(img_path)[...,::-1]\n",
    "    image = cv2.flip(image,0)\n",
    "    #plt.imshow(image)\n",
    "    plt.imsave(os.path.join(write_folder,img),image)\n",
    "    del(image)\n",
    "    #break\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "%matplotlib inline\n",
    "\n",
    "import cv2\n",
    "from mtcnn import MTCNN\n",
    "#from google.colab.patches import cv2_imshow\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "detector = MTCNN()\n",
    "\n",
    "read_path = r\"C:\\Users\\Lenovo\\Desktop\\Project\\DataX-2\\Raw Data\\top_view_corrected\"\n",
    "#read_path = r\"C:\\Users\\Lenovo\\Desktop\\Project\\DataX-2\\Raw Data\\test\"\n",
    "\n",
    "write_path = r\"C:\\Users\\Lenovo\\Desktop\\Project\\DataX-2\\Raw Data\\top_view_corrected_face_detected\" #path shud exist!\n",
    "#write_path = read_path+\"_face_detected\"\n",
    "\n",
    "\n",
    "for img in os.listdir(read_path):\n",
    "    img_path = os.path.join(read_path,img)\n",
    "    #image = cv2.imread(img_path)[...,::-1] #this doesnt seem to work!\n",
    "    image = cv2.cvtColor(cv2.imread(img_path), cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    result = detector.detect_faces(image)\n",
    "    #print(result)\n",
    "    \n",
    "    i=0\n",
    "    for objects in result:\n",
    "        bounding_box = objects['box']\n",
    "        #keypoints = objects['keypoints']\n",
    "        \n",
    "        crop_img = image[bounding_box[1]:bounding_box[1]+bounding_box[3], bounding_box[0]:bounding_box[0]+bounding_box[2]]\n",
    "        \n",
    "        directory = os.path.join(write_path,img.split(\".\")[0])\n",
    "        if not os.path.exists(directory):\n",
    "            os.makedirs(directory)\n",
    "            \n",
    "        new_path = os.path.join(directory,img.split(\".\")[0]+\"_\"+str(i)+\".jpg\")\n",
    "        plt.imsave(new_path,crop_img)\n",
    "        i+=1\n",
    "        \n",
    "        cv2.rectangle(image,\n",
    "                  (bounding_box[0], bounding_box[1]),\n",
    "                  (bounding_box[0]+bounding_box[2], bounding_box[1] + bounding_box[3]),\n",
    "                  (0,155,255),2)\n",
    "        \n",
    "    #plt.imshow(image)\n",
    "    plt.imsave(os.path.join(write_path,img),image)\n",
    "    \n",
    "    #break\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Only 1 time, ever!\n",
    "\n",
    "\n",
    "import bz2\n",
    "import os\n",
    "\n",
    "from urllib.request import urlopen\n",
    "\n",
    "def download_landmarks(dst_file):\n",
    "    url = 'http://dlib.net/files/shape_predictor_68_face_landmarks.dat.bz2'\n",
    "    decompressor = bz2.BZ2Decompressor()\n",
    "    \n",
    "    with urlopen(url) as src, open(dst_file, 'wb') as dst:\n",
    "        data = src.read(1024)\n",
    "        while len(data) > 0:\n",
    "            dst.write(decompressor.decompress(data))\n",
    "            data = src.read(1024)\n",
    "\n",
    "dst_dir = 'models'\n",
    "dst_file = os.path.join(dst_dir, 'landmarks.dat')\n",
    "\n",
    "if not os.path.exists(dst_file):\n",
    "    os.makedirs(dst_dir)\n",
    "    download_landmarks(dst_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Not needed for inference \n",
    "\n",
    "\"\"\"\n",
    "from keras import backend as K\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Layer\n",
    "\n",
    "# Input for anchor, positive and negative images\n",
    "in_a = Input(shape=(96, 96, 3))\n",
    "in_p = Input(shape=(96, 96, 3))\n",
    "in_n = Input(shape=(96, 96, 3))\n",
    "\n",
    "# Output for anchor, positive and negative embedding vectors\n",
    "# The nn4_small model instance is shared (Siamese network)\n",
    "emb_a = nn4_small2(in_a)\n",
    "emb_p = nn4_small2(in_p)\n",
    "emb_n = nn4_small2(in_n)\n",
    "\n",
    "class TripletLossLayer(Layer):\n",
    "    def __init__(self, alpha, **kwargs):\n",
    "        self.alpha = alpha\n",
    "        super(TripletLossLayer, self).__init__(**kwargs)\n",
    "    \n",
    "    def triplet_loss(self, inputs):\n",
    "        a, p, n = inputs\n",
    "        p_dist = K.sum(K.square(a-p), axis=-1)\n",
    "        n_dist = K.sum(K.square(a-n), axis=-1)\n",
    "        return K.sum(K.maximum(p_dist - n_dist + self.alpha, 0), axis=0)\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        loss = self.triplet_loss(inputs)\n",
    "        self.add_loss(loss)\n",
    "        return loss\n",
    "\n",
    "# Layer that computes the triplet loss from anchor, positive and negative embedding vectors\n",
    "triplet_loss_layer = TripletLossLayer(alpha=0.2, name='triplet_loss_layer')([emb_a, emb_p, emb_n])\n",
    "#alpha is the hyper-parameter\n",
    "\n",
    "# Model that can be trained with anchor, positive negative images\n",
    "nn4_small2_train = Model([in_a, in_p, in_n], triplet_loss_layer)\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "ename": "NotADirectoryError",
     "evalue": "[WinError 267] The directory name is invalid: 'C:\\\\Users\\\\Lenovo\\\\Desktop\\\\Project\\\\DataX-2\\\\labelled Data\\\\Akbar.jpg'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNotADirectoryError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-36-d7629d2a63e0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[0mlabelled_path\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34mr\"C:\\Users\\Lenovo\\Desktop\\Project\\DataX-2\\labelled Data\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 31\u001b[1;33m \u001b[0mclasses\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload_metadata\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabelled_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-36-d7629d2a63e0>\u001b[0m in \u001b[0;36mload_metadata\u001b[1;34m(path)\u001b[0m\n\u001b[0;32m     20\u001b[0m     \u001b[0mmetadata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msorted\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m         \u001b[1;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msorted\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     23\u001b[0m             \u001b[1;31m# Check file extension. Allow only jpg/jpeg' files.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m             \u001b[0mext\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplitext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNotADirectoryError\u001b[0m: [WinError 267] The directory name is invalid: 'C:\\\\Users\\\\Lenovo\\\\Desktop\\\\Project\\\\DataX-2\\\\labelled Data\\\\Akbar.jpg'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os.path\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "class IdentityMetadata():\n",
    "    def __init__(self, base, name, file):\n",
    "        # dataset base directory\n",
    "        self.base = base\n",
    "        # identity name\n",
    "        self.name = name\n",
    "        # image file name\n",
    "        self.file = file\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.image_path()\n",
    "\n",
    "    def image_path(self):\n",
    "        return os.path.join(self.base, self.name, self.file) \n",
    "    \n",
    "def load_metadata(path):\n",
    "    metadata = []\n",
    "    for i in sorted(os.listdir(path)):\n",
    "        for f in sorted(os.listdir(os.path.join(path, i))):\n",
    "            # Check file extension. Allow only jpg/jpeg' files.\n",
    "            ext = os.path.splitext(f)[1]\n",
    "            if ext == '.jpg' or ext == '.jpeg':\n",
    "                metadata.append(IdentityMetadata(path, i, f))\n",
    "    return np.array(metadata)\n",
    "\n",
    "\n",
    "labelled_path = r\"C:\\Users\\Lenovo\\Desktop\\Project\\DataX-2\\labelled Data\"\n",
    "classes = load_metadata(labelled_path)\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "\n",
    "import cv2\n",
    "from align import AlignDlib\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "def load_image(path):\n",
    "    img = cv2.imread(path, 1)\n",
    "    # OpenCV loads images with color channels\n",
    "    # in BGR order. So we need to reverse them\n",
    "    return img[...,::-1]\n",
    "\n",
    "# Initialize the OpenFace face alignment utility\n",
    "alignment = AlignDlib('models/landmarks.dat')\n",
    "\n",
    "# Load an image of Jacques Chirac\n",
    "jc_orig = load_image(metadata[1].image_path())\n",
    "\n",
    "#print(type(jc_orig))\n",
    "\n",
    "# Detect face and return bounding box\n",
    "bb = alignment.getLargestFaceBoundingBox(jc_orig)\n",
    "\n",
    "# Transform image using specified face landmark indices and crop image to 96x96\n",
    "jc_aligned = alignment.align(96, jc_orig, bb, landmarkIndices=AlignDlib.OUTER_EYES_AND_NOSE)\n",
    "\n",
    "# Show original image\n",
    "plt.subplot(131)\n",
    "plt.imshow(jc_orig)\n",
    "\n",
    "# Show original image with bounding box\n",
    "plt.subplot(132)\n",
    "plt.imshow(jc_orig)\n",
    "plt.gca().add_patch(patches.Rectangle((bb.left(), bb.top()), bb.width(), bb.height(), fill=False, color='red'))\n",
    "\n",
    "# Show aligned image\n",
    "plt.subplot(133)\n",
    "plt.imshow(jc_aligned);\n",
    "\n",
    "\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "from align import AlignDlib\n",
    "from model import create_model\n",
    "\n",
    "\n",
    "#alignment = AlignDlib('models/landmarks.dat')\n",
    "def align_image(img,alignment):\n",
    "    return alignment.align(96, img, alignment.getLargestFaceBoundingBox(img), \n",
    "                           landmarkIndices=AlignDlib.OUTER_EYES_AND_NOSE)\n",
    "\n",
    "#img is numpy array of any size!\n",
    "#output is 96*96*3 for FaceNet to use.\n",
    "\n",
    "#uses  AlignDlib utility from the OpenFace\n",
    "\n",
    "nn4_small2_pretrained = create_model()\n",
    "nn4_small2_pretrained.load_weights('weights/nn4.small2.v1.h5')\n",
    "\n",
    "#MODEL PRETRAINED ON combinatio of:\n",
    "# 10,000 subjects and 500,000 images, called CASIAWebFace\n",
    "# 100,000 Face Images of 530 People. male female balanced called FaceScrub\n",
    "\n",
    "#200 million images of 8 million identities\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "root=\"test_images\\Extracted_faces\"\n",
    "os.listdir(root)\n",
    "\n",
    "\n",
    "input_data = []\n",
    "\n",
    "\n",
    "for image in os.listdir(root):\n",
    "    if img[-3:] == \"jpg\" or img[-3:] == \"png\" or img[-3:] == \"jpeg\":\n",
    "        cv2.imread(\"\")\n",
    "    \n",
    "\n",
    "\n",
    "class_folders=sorted(os.listdir(root))\n",
    "\n",
    "for clas in sorted(os.listdir(\"test_images\")):\n",
    "    for im in os.listdir(os.path.join(root,clas)):\n",
    "        #clas_images=sorted(os.listdir(clas))[i]\n",
    "        #print(im)\n",
    "        metadata.append(IdentityMetadata(root, clas, im))\n",
    "        \n",
    "        \n",
    "    #images_list[i].append(clas_images)\n",
    "    #metadata.append(IdentityMetadata(root, i, f))\n",
    "    \n",
    "metadata=np.array(metadata)\n",
    "#path=Path(os.path.join(os.getcwd(),\"test_images\",class_folders[0]),\"s1.jpg\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from align import AlignDlib\n",
    "import random\n",
    "\n",
    "\n",
    "def distance(emb1, emb2):\n",
    "    return np.sum(np.square(emb1 - emb2))\n",
    "\n",
    "\n",
    "def matcher(input_embedding,classes,threshold):\n",
    "    min_distance = 100.0\n",
    "    target = -1\n",
    "    for i in range(classes.shape[0]):\n",
    "        if distance(input_embedding,classes[i]) < min_distance:\n",
    "            min_distance = distance(input_embedding,classes[i])\n",
    "            target = i\n",
    "            \n",
    "    if min_distance > threshold: #model unable to classify\n",
    "        return (-1,round(min_distance,2))\n",
    "        \n",
    "    return (target,round(min_distance,2))\n",
    "\n",
    "    \n",
    "\n",
    "def pipeline(input_path,classes,result_path,class_names,threshold=0.8,random_input=30):\n",
    "    # Initialize the OpenFace face alignment utility\n",
    "    alignment = AlignDlib('models/landmarks.dat')\n",
    "    \n",
    "    input_dim = 96\n",
    "\n",
    "    #X_train = np.ones(shape=(tot,input_dim,input_dim),dtype=np.uint8)\n",
    "\n",
    "    #random_input = 100\n",
    "    input_items = os.listdir(input_path)\n",
    "    input_folders = filter(lambda x: x[-3:] !=\"jpg\", input_items) \n",
    "    input_folders = list(input_folders)\n",
    "    \n",
    "    tot = len(input_folders)\n",
    "    if tot > random_input:\n",
    "        #input_folders = random.sample(input_folders,random_input)\n",
    "        input_folders = input_folders[:random_input]\n",
    "    \n",
    "    #embedded = np.zeros((tot, 128))\n",
    "    \n",
    "    k = len(input_folders)\n",
    "    for j,folder in enumerate(input_folders):\n",
    "        print(\"Image %d out of %d\" %(j,k))\n",
    "        #anything reset?\n",
    "        folder_path = os.path.join(input_path,folder)\n",
    "        print(folder_path)\n",
    "        for i,file in enumerate(os.listdir(folder_path)):\n",
    "            \n",
    "            im = cv2.imread(os.path.join(folder_path,file))\n",
    "            \n",
    "            im = im[...,::-1]\n",
    "            #im_new = cv2.resize(im, (96,96), interpolation=cv2.INTER_LINEAR) #cubic increase all the values!\n",
    "            \n",
    "            im_new = align_image(im,alignment) #also changes dimension to std 96*96\n",
    "            \n",
    "            if np.any(im_new) == None: #unable to align image\n",
    "                continue\n",
    "                \n",
    "            #print(im.shape,im_new.shape)\n",
    "            \n",
    "            #src, dst, dst.size(), 0, 0, interpolation\n",
    "            #X_train[i] = im_new\n",
    "            im_new = (im_new / 255.).astype(np.float32)\n",
    "            embedding = nn4_small2_pretrained.predict(np.expand_dims(im_new, axis=0))[0]\n",
    "\n",
    "            target,distance = matcher(embedding,classes,threshold)\n",
    "            \n",
    "            if target == -1: #model unable to classify\n",
    "                if not os.path.exists(os.path.join(result_path,\"unclassified\",folder)):\n",
    "                    os.makedirs(os.path.join(result_path,\"unclassified\",folder))\n",
    "                \n",
    "                plt.imsave(os.path.join(os.path.join(result_path,\"unclassified\",folder),file[:-4]+str(distance)+\".jpg\"),im)\n",
    "            else:\n",
    "                if not os.path.exists(os.path.join(result_path,folder)):\n",
    "                    os.makedirs(os.path.join(result_path,folder))\n",
    "                    \n",
    "                plt.imsave(os.path.join(os.path.join(result_path,folder),class_names[target]+str(distance)+\".jpg\"),im)\n",
    "            \n",
    "            #print(target,distance)\n",
    "    \n",
    "    #return embedded\n",
    "    #alignment.align(96, img, alignment.getLargestFaceBoundingBox(img),landmarkIndices=AlignDlib.OUTER_EYES_AND_NOSE)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image 0 out of 10\n",
      "C:\\Users\\Lenovo\\Desktop\\Project\\DataX-2\\Raw Data\\top_view_corrected_face_detected\\000005\n",
      "(44, 31, 3) (96, 96, 3)\n",
      "(32, 27, 3) (96, 96, 3)\n",
      "(50, 43, 3) (96, 96, 3)\n",
      "(36, 30, 3) (96, 96, 3)\n",
      "Image 1 out of 10\n",
      "C:\\Users\\Lenovo\\Desktop\\Project\\DataX-2\\Raw Data\\top_view_corrected_face_detected\\000007\n",
      "(31, 28, 3) (96, 96, 3)\n",
      "(38, 30, 3) (96, 96, 3)\n",
      "(68, 53, 3) (96, 96, 3)\n",
      "(49, 39, 3) (96, 96, 3)\n",
      "Image 2 out of 10\n",
      "C:\\Users\\Lenovo\\Desktop\\Project\\DataX-2\\Raw Data\\top_view_corrected_face_detected\\000008\n",
      "(47, 40, 3) (96, 96, 3)\n",
      "(35, 30, 3) (96, 96, 3)\n",
      "(32, 29, 3) (96, 96, 3)\n",
      "(53, 42, 3) (96, 96, 3)\n",
      "(38, 31, 3) (96, 96, 3)\n",
      "(51, 40, 3) (96, 96, 3)\n",
      "(37, 30, 3) (96, 96, 3)\n",
      "(47, 38, 3) (96, 96, 3)\n",
      "Image 3 out of 10\n",
      "C:\\Users\\Lenovo\\Desktop\\Project\\DataX-2\\Raw Data\\top_view_corrected_face_detected\\000009\n",
      "(68, 48, 3) (96, 96, 3)\n",
      "(75, 64, 3) (96, 96, 3)\n",
      "(32, 29, 3) (96, 96, 3)\n",
      "(69, 51, 3) (96, 96, 3)\n",
      "(50, 40, 3) (96, 96, 3)\n",
      "(39, 31, 3) (96, 96, 3)\n",
      "Image 4 out of 10\n",
      "C:\\Users\\Lenovo\\Desktop\\Project\\DataX-2\\Raw Data\\top_view_corrected_face_detected\\000010\n",
      "(34, 29, 3) (96, 96, 3)\n",
      "(51, 38, 3) (96, 96, 3)\n",
      "(53, 42, 3) (96, 96, 3)\n",
      "(50, 41, 3) (96, 96, 3)\n",
      "(61, 49, 3) (96, 96, 3)\n",
      "(70, 51, 3) (96, 96, 3)\n",
      "Image 5 out of 10\n",
      "C:\\Users\\Lenovo\\Desktop\\Project\\DataX-2\\Raw Data\\top_view_corrected_face_detected\\000011\n",
      "(38, 30, 3) (96, 96, 3)\n",
      "(50, 40, 3) (96, 96, 3)\n",
      "(51, 40, 3) (96, 96, 3)\n",
      "(34, 28, 3) (96, 96, 3)\n",
      "(34, 30, 3) (96, 96, 3)\n",
      "(72, 51, 3) (96, 96, 3)\n",
      "(56, 43, 3) (96, 96, 3)\n",
      "Image 6 out of 10\n",
      "C:\\Users\\Lenovo\\Desktop\\Project\\DataX-2\\Raw Data\\top_view_corrected_face_detected\\000012\n",
      "(47, 39, 3) (96, 96, 3)\n",
      "(73, 52, 3) (96, 96, 3)\n",
      "(54, 45, 3) (96, 96, 3)\n",
      "(35, 29, 3) (96, 96, 3)\n",
      "(54, 42, 3) (96, 96, 3)\n",
      "(36, 30, 3) (96, 96, 3)\n",
      "(49, 40, 3) (96, 96, 3)\n",
      "(47, 38, 3) (96, 96, 3)\n",
      "(37, 31, 3) (96, 96, 3)\n",
      "Image 7 out of 10\n",
      "C:\\Users\\Lenovo\\Desktop\\Project\\DataX-2\\Raw Data\\top_view_corrected_face_detected\\000013\n",
      "(50, 44, 3) (96, 96, 3)\n",
      "(37, 29, 3) (96, 96, 3)\n",
      "(43, 31, 3) (96, 96, 3)\n",
      "(56, 45, 3) (96, 96, 3)\n",
      "(52, 40, 3) (96, 96, 3)\n",
      "(51, 42, 3) (96, 96, 3)\n",
      "Image 8 out of 10\n",
      "C:\\Users\\Lenovo\\Desktop\\Project\\DataX-2\\Raw Data\\top_view_corrected_face_detected\\000014\n",
      "(39, 30, 3) (96, 96, 3)\n",
      "(35, 28, 3) (96, 96, 3)\n",
      "(71, 54, 3) (96, 96, 3)\n",
      "(51, 40, 3) (96, 96, 3)\n",
      "(44, 30, 3) (96, 96, 3)\n",
      "(54, 45, 3) (96, 96, 3)\n",
      "(50, 43, 3) (96, 96, 3)\n",
      "(38, 29, 3) (96, 96, 3)\n",
      "Image 9 out of 10\n",
      "C:\\Users\\Lenovo\\Desktop\\Project\\DataX-2\\Raw Data\\top_view_corrected_face_detected\\000015\n",
      "(72, 53, 3) (96, 96, 3)\n",
      "(36, 30, 3) (96, 96, 3)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "labelled_path = r\"C:\\Users\\Lenovo\\Desktop\\Project\\DataX-2\\labelled Data\"\n",
    "input_path = r\"C:\\Users\\Lenovo\\Desktop\\Project\\DataX-2\\Raw Data\\top_view_corrected_face_detected\" #dont read images from here!\n",
    "result_path = r\"C:\\Users\\Lenovo\\Desktop\\Project\\DataX-2\\Results\"\n",
    "\n",
    "class_names = os.listdir(r\"C:\\Users\\Lenovo\\Desktop\\Project\\DataX-2\\labelled Data\")\n",
    "class_names = [item[:-4] for item in class_names]\n",
    "\n",
    "embedded_test = pipeline(input_path,classes_embedded,result_path,class_names,threshold=0.8,random_input=10)\n",
    "#\"test_images\\Extracted_faces\"\n",
    "\n",
    "#.43 wrong snehdeep classified as Manish, .33 vinay as anurag\n",
    "#on the ones it can classify: 11/15 \n",
    "\n",
    "#total:194. classified:53,unclassified:7. means couldn't align: 134!\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Akbar',\n",
       " 'Aman',\n",
       " 'Anisha',\n",
       " 'Anshuman',\n",
       " 'Anurag',\n",
       " 'Balbir',\n",
       " 'Bharadeep',\n",
       " 'Hemant',\n",
       " 'Kartik',\n",
       " 'Manan',\n",
       " 'Manish',\n",
       " 'Ojas',\n",
       " 'Ojasvi',\n",
       " 'Pradeep',\n",
       " 'Praveen',\n",
       " 'Priyanka',\n",
       " 'Ramya',\n",
       " 'Ravinder_LI',\n",
       " 'Sai',\n",
       " 'Saif',\n",
       " 'Shrey Arora',\n",
       " 'Snehdeep',\n",
       " 'Tushar',\n",
       " 'Vinay']"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
